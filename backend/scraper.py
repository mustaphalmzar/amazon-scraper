import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
import random
from difflib import SequenceMatcher
from user_agent_generator import generate_user_agent  # âœ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù€ User-Agent

# âœ… ÙƒÙ„Ù…Ø§Øª ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„ÙŠÙ‡Ø§
EXCLUDED_KEYWORDS = [
    'long sleeve', 'hoodie', 'sweatshirt', 'v-neck', 'vinyl shirt',
    'premium t-shirt', 'tank top', 'polo', 'raglan', 'shorts'
]

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
COOKIES = {
    "lc-main": "en_US",
    "i18n-prefs": "USD",
    "session-id": "130-7384185-5606739",
    "ubid-main": "131-5391624-1279526"
}

# âœ… Ù…Ù‚Ø§Ø±Ù†Ø© ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø© (Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø£Ø®Ø·Ø§Ø¡ Ø¨Ø³ÙŠØ·Ø©)
def is_similar(a, b, threshold=0.7):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold

# âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬
import re

async def fetch_product_details(session, asin):
    try:
        url = f"https://www.amazon.com/dp/{asin}"
        async with session.get(url, headers=generate_user_agent(), cookies=COOKIES, timeout=10) as response:
            content = await response.text()
            soup = BeautifulSoup(content, 'html.parser')

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ BSR
            bsr_list = []
            bsr_tags = soup.find_all('span', string=re.compile('Best Sellers Rank'))
            for tag in bsr_tags:
                bsr_text = tag.find_next('span').get_text(strip=True)
                bsr_matches = re.findall(r'#\d{1,3}(?:,\d{3})*', bsr_text)
                bsr_list.extend([bsr.replace('#', '') for bsr in bsr_matches])

            if not bsr_list:
                bsr_list = ['N/A']  # Ø¹Ø±Ø¶ N/A Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ BSR

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
            ratings_count_tag = soup.select_one('#acrCustomerReviewText')
            ratings_count = ratings_count_tag.text.split()[0] if ratings_count_tag else '0'

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±
            date_tag = soup.find('span', string=re.compile('Date First Available'))
            date_published = date_tag.find_next('span').text.strip() if date_tag else 'N/A'

            return bsr_list, ratings_count, date_published

    except Exception as e:
        print(f"âŒ Error fetching product details for ASIN {asin}: {e}")
        return ['N/A'], '0', 'N/A'  # Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø§Ù‚ØµØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù†ØªØ¬

async def fetch_search_results(session, keyword, page):
    search_url = (
        f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}"
        f"&rh=n%3A7141123011%2Cp_6%3AATVPDKIKX0DER"
        f"&s=exact-aware-popularity-rank&dc&page={page}"
    )

    try:
        await asyncio.sleep(random.uniform(1, 3))  # ØªØ£Ø®ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        async with session.get(search_url, headers=generate_user_agent(), cookies=COOKIES, timeout=15) as response:
            content = await response.text()
            soup = BeautifulSoup(content, 'html.parser')
            return soup.find_all('div', {'data-asin': True})

    except Exception as e:
        print(f"âŒ Error fetching page {page}: {e}")
        return []

# âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù†ØªØ¬
from nltk.corpus import wordnet  # âœ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…ÙƒØªØ¨Ø© WordNet Ù„Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª

# âœ… Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¯ÙØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø©
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name().lower())
    return synonyms

# âœ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
async def process_product(session, div, keyword):
    asin = div.get('data-asin')
    if not asin:
        return None

    title_tag = div.select_one('h2 a span') or div.select_one('.a-size-base-plus.a-color-base.a-text-normal')
    title = title_tag.text.strip() if title_tag else 'N/A'

    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¸ÙˆØ±Ø©
    if any(word.lower() in title.lower() for word in EXCLUDED_KEYWORDS):
        return None

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£Ùˆ Ù…Ø±Ø§Ø¯ÙØ§ØªÙ‡Ø§
    required_keywords = keyword.lower().split()
    title_words = title.lower().split()

    for word in required_keywords:
        synonyms = get_synonyms(word)  # âœ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
        synonyms.add(word)  # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª

        if not any(any(is_similar(synonym, title_word) for title_word in title_words) for synonym in synonyms):
            return None

    image_tag = div.select_one('img.s-image')
    image = image_tag['src'] if image_tag else ''

    product_link_tag = div.select_one('h2 a')
    product_link = f"https://www.amazon.com{product_link_tag['href']}" if product_link_tag else f"https://www.amazon.com/dp/{asin}"

    bsr, ratings, date_published = await fetch_product_details(session, asin)

    return {
        'asin': asin,
        'title': title,
        'bsr': bsr,
        'ratings': ratings,
        'date_published': date_published,
        'image': image,
        'link': product_link
    }


# âœ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£Ù…Ø§Ø²ÙˆÙ†
async def scrape_amazon(keyword, pages=1):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for page in range(1, pages + 1):
            print(f"ğŸ” Fetching page {page}...")
            product_containers = await fetch_search_results(session, keyword, page)
            print(f"âœ… Page {page}: Found {len(product_containers)} products.")

            for div in product_containers:
                tasks.append(process_product(session, div, keyword))

        products = await asyncio.gather(*tasks)
        products = [product for product in products if product]  # Ø­Ø°Ù Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ§Ø±ØºØ©

        print(f"ğŸš€ Total products extracted: {len(products)}")
        return products

# âœ… Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
def run_scraper(keyword, pages=1):
    return asyncio.run(scrape_amazon(keyword, pages))
